{"id": "0", "title": "Large language model", "source": "https://en.wikipedia.org/wiki/Large_language_model", "text": "A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\n\n\n== History ==\n\nBefore the emergence of transformer-based models in 2017, some language models were considered large relative to the computational and data constraints of their time. In the early 1990s, IBM's statistical models pioneered word alignment techniques for machine translation, laying the groundwork for corpus-based language modeling. A smoothed n-gram model in 2001, such as those employing Kneser-Ney smoothing, trained on 300 million words achieved state-of-the-art perplexity on benchmark tests at the time. During the 2000s, with the rise of widespread internet access, researchers began compiling massive text datasets from the web (\"web as corpus\") to train statistical language models.\n\nMoving beyond n-gram models, researchers started in 2000 to use neural networks to learn language models. Following the breakthrough of deep neural networks in image classification around 2012, similar architectures were adapted for language tasks. This shift was marked by the development of word embeddings (eg, Word2Vec by Mikolov in 2013) and sequence-to-sequence (seq2seq) models using LSTM. In 2016, Google transitioned its translation service to neural machine translation (NMT), replacing statistical phrase-based models with deep recurrent neural networks. These early NMT systems used LSTM-based encoder-decoder architectures, as they preceded the invention of transformers. \nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper's goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2025 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing chatbot ChatGPT that received extensive media coverage and public attention. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer. Many LLMs with parameter counts comparable to those of OpenAI's GPT series have been developed.\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache Licens"}
{"id": "1", "title": "List of large language models", "source": "https://en.wikipedia.org/wiki/List_of_large_language_models", "text": "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\nThis page lists notable large language models.\n\n\n== List ==\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.\n\n\n== See also ==\nList of chatbots\nList of language model benchmarks\n\n\n== Notes ==\n\n\n== References =="}
{"id": "2", "title": "Llama (language model)", "source": "https://en.wikipedia.org/wiki/Llama_(language_model)", "text": "Llama (Large Language Model Meta AI) is a family of large language models (LLMs) released by Meta AI starting in February 2023. The latest version is Llama 4, released in April 2025.\nLlama models come in different sizes, ranging from 1 billion to 2 trillion parameters. Initially only a foundation model, starting with Llama 2, Meta AI released instruction fine-tuned versions alongside foundation models.\nModel weights for the first version of Llama were only available to researchers on a case-by-case basis, under a non-commercial license. Unauthorized copies of the first model were shared via BitTorrent. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use.\nAlongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\n\n\n== Background ==\nAfter the release of large language models such as GPT-3, a focus of research was up-scaling models, which in some instances showed major increases in emergent capabilities. The release of ChatGPT and its surprise success caused an increase in attention to large language models.\nCompared with other responses to ChatGPT, Meta's Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.\nAn empirical investigation of the Llama series was the scaling laws. It was observed that the Llama 3 models showed that when a model is trained on data that is more than the \"Chinchilla-optimal\" amount, the performance continues to scale log-linearly. For example, the Chinchilla-optimal dataset for Llama 3 8B is 200 billion tokens, but performance continued to scale log-linearly to the 75-times larger dataset of 15 trillion tokens.\n\n\n== Initial release ==\nThe first version of Llama (stylized as LLaMA and sometimes referred to as Llama 1) was announced on February 24, 2023, via a blog post and a paper describing the model's training, architecture, and performance. The inference code used to run the model was publicly released under the open-source GPLv3 license. Access to the model's weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".\nLlama was trained on only publicly available information, and was trained at various model sizes, with the intention to make it more accessible to different hardware. The model was exclusively a foundation model, although the paper contained examples of instruction fine-tuned versions of the model.\nMeta AI reported the 13B parameter model performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters), and the largest 65B model was competitive with state of the art models such as PaLM and Chinchilla.\n\n\n=== Leak ===\nOn March 3, 2023, a torrent containing Llama's weights was uploaded, with a link to the torrent shared on the 4chan imageboard and subsequently spread through online AI communities. That same day, a pull request on the main Llama repository was opened, requesting to add the magnet link to the official documentation. On March 4, a pull request was opened to add links to HuggingFace repositories containing the model. On March 6, Meta filed takedown requests to remove the HuggingFace repositories linked in the pull request, characterizing it as \"unauthorized distribution\" of the model. HuggingFace complied with the requests. On March 20, Meta filed a DMCA takedown request for copyright infringement against a repository containing a script that downloaded Llama from a mirror, and GitHub complied the next day.\nReactions to the leak varied. Some speculated that the model would be used for malicious purposes, such as more sophisticated spam. Some have celebrated the model's accessibility, as well as the fact that smaller"}
{"id": "3", "title": "Language model", "source": "https://en.wikipedia.org/wiki/Language_model", "text": "A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\n\n\n== History ==\nNoam Chomsky did pioneering work on language models in the 1950s by developing a theory of formal grammars.\nIn 1980, statistical approaches were explored and found to be more useful for many purposes than rule-based formal grammars. Discrete representations like word n-gram language models, with probabilities for discrete combinations of words, made significant advances.\nIn the 2000s, continuous representations for words, such as word embeddings, began to replace discrete representations. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning, and common relationships between pairs of words like plurality or gender.\n\n\n== Pure statistical models ==\nIn 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\n\n\n=== Models based on word n-grams ===\n\n\n=== Exponential ===\nMaximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The equation is\n\n  \n    \n      \n        P\n        (\n        \n          w\n          \n            m\n          \n        \n        ∣\n        \n          w\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n            −\n            1\n          \n        \n        )\n        =\n        \n          \n            1\n            \n              Z\n              (\n              \n                w\n                \n                  1\n                \n              \n              ,\n              …\n              ,\n              \n                w\n                \n                  m\n                  −\n                  1\n                \n              \n              )\n            \n          \n        \n        exp\n        ⁡\n        (\n        \n          a\n          \n            T\n          \n        \n        f\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle P(w_{m}\\mid w_{1},\\ldots ,w_{m-1})={\\frac {1}{Z(w_{1},\\ldots ,w_{m-1})}}\\exp(a^{T}f(w_{1},\\ldots ,w_{m}))}\n  \n\nwhere \n  \n    \n      \n        Z\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n            −\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle Z(w_{1},\\ldots ,w_{m-1})}\n  \n is the partition function, \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is the parameter vector, and \n  \n    \n      \n        f\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle f(w_{1},\\ldots ,w_{m})}\n  \n is the feature function. In the simplest case, the feature function is just an ind"}
{"id": "4", "title": "Reasoning language model", "source": "https://en.wikipedia.org/wiki/Reasoning_language_model", "text": "Reasoning language models (RLMs) are large language models that are trained further to solve tasks that take several steps of reasoning. They tend to do better on logic, math, and programming tasks than standard LLMs, can revisit and revise earlier steps, and make use of extra computation while answering as another way to scale performance, alongside the number of training examples, parameters, and training compute.\n\n\n== History ==\n\n\n=== 2024 ===\nIn September 2024, OpenAI released o1-preview, an LLM with enhanced reasoning. The full version, o1, followed in December 2024. OpenAI also began sharing results on its successor, o3.\nThe development of reasoning LLMs has illustrated what Rich Sutton called the \"bitter lesson\": that scaling compute often outperforms methods that rely on specific human insights. For example, the Generative AI Research Lab (GAIR) explored complex methods such as tree search and reinforcement learning to replicate o1's capabilities. In their \"o1 Replication Journey\" papers they reported that knowledge distillation (training a smaller model to imitate o1's outputs) worked surprisingly well. This highlighted the effectiveness of distillation in this context.\nAlibaba released reasoning versions of its Qwen LLMs in November 2024. \nIn December 2024, the team introduced QvQ-72B-Preview, an experimental visual reasoning model.\nIn December 2024, Google introduced Deep Research in Gemini, a feature that runs multi-step research tasks.\nOn December 16, 2024, an experiment with a Llama 3B model showed that by scaling test-time compute, a relatively small model could outperform a much larger Llama 70B model on challenging reasoning tasks. This suggested that better inference strategies can unlock useful reasoning capabilities even in small models.\n\n\n=== 2025 ===\nIn January 2025, DeepSeek released R1, a model with comparable performance to o1 at lower cost. The release demonstrated the effectiveness of Group Relative Policy Optimization (GRPO). On January 25, 2025, DeepSeek added a feature to DeepSeek R1 that lets the model search the web while it reasons, making it easier to combine retrieval with reasoning. The effectiveness of distillation for reasoning models was shown in works such as s1-32B, which achieved strong performance through budget forcing and scaling methods.\nOn February 2, 2025, OpenAI released Deep Research based on their o3 model, allowing users to initiate complex research tasks and generate comprehensive reports which incorporate various sources from the web.\n\n\n== Supervised finetuning ==\nA large language model (LLM) can be fine-tuned on a dataset of reasoning tasks paired with example solutions and step-by-step (reasoning) traces. The fine-tuned model can then produce its own reasoning traces for new problems.\nBecause human-written traces are costly to collect, researchers have proposed ways to build such datasets automatically. In rejection sampling finetuning (RFT), new reasoning traces are gathered in a loop:\n\nSample a task prompt.\nGenerate many reasoning traces for the prompt.\nUse a verifier to remove reasoning traces with a wrong final answer, and optionally remove duplicates\n\n\n== Reinforcement learning ==\nA pretrained language model can be further trained with RL. In the RL formalism, a generative language model is a policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n. A task prompt is an environmental state \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, and the model's response is an action \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n. The probability that the model responds \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n with \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n is \n  \n    \n      \n        π\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle \\pi (y|x)}\n  \n.\nTraining a reasoning language model with RL means constructing a reward model"}
{"id": "5", "title": "Large language models in government", "source": "https://en.wikipedia.org/wiki/Large_language_models_in_government", "text": "Large language models have been used by officials and politicians in a wide variety of ways.\n\n\n== Overview ==\nThe Conversation described ChatGPT as a \"uniquely terrible tool\" for government ministers.\nGoogle released certain details of usage of Gemini by the governments of Iran, China, Russia and North Korea.\n\n\n== Details by country ==\n\n\n=== Australia ===\nThe Australian Government has not issued a comprehensive directive on generative AI usage, leaving decisions to individual departments. In 2023, the Department of Home Affairs allowed ChatGPT use in limited circumstances, while the Australian Federal Police blocked it.\n\n\n==== States and territories ====\nEvery state and territory, except South Australia, restricted the use of ChatGPT in public schools. In 2024, NSWEduChat was rolled out to replace ChatGPT.\n\n\n=== Austria ===\nIn 2024, Austria used a chatbot based on ChatGPT to answer questions of the recipients of welfare.\n\n\n==== Vienna ====\nThe Viennese government used ChatGPT to write an anthem for the city-state.\n\n\n=== Brazil ===\nCity lawmakers in Porto Alegre enacted an ordinance, which was largely written using ChatGPT.\n\n\n=== Germany ===\nAs of 2024, use of ChatGPT varied considerably between different federal ministries.\n\n\n==== Schleswig-Holstein ====\nThe digitalisation minister, Dirk Schrödter, announced that the government of Schleswig-Holstein would use ChatGPT in its administration.\n\n\n=== India ===\nIn 2025, the Ministry of Finance banned its employees from using ChatGPT and DeepSeek on government devices.\n\n\n=== Israel ===\nIn February 2023, the president of Israel Isaac Herzog delivered a speech that had partially been written using ChatGPT.\nIn March 2025, reporting by +972 Magazine revealed the development of a large language model by Unit 8200, an intelligence unit of the Israeli Defence Forces.\n\n\n=== Japan ===\nIn January 2025, the Japanese Government launched a large language model tool to help doctors in diagnosing patients.\n\n\n=== Korea ===\nIn February 2025, the Korean governemnt announced a plan to develop a Korean LLM with an investment of approximately ₩1,000,000,000,000.\n\n\n=== New Zealand ===\nIn October 2024, the New Zealand Government launched its GovGPT pilot.\n\n\n=== Poland ===\nIn February 2025, the Polish government announced the launch of PLLuM, the Polish Large Language Model, designed to specialise in content in the Polish language.\n\n\n=== United Kingdom ===\nIn March 2025, the New Scientist revealed it had obtained science minister Peter Kyle's ChatGPT prompts.  The topics of Kyle's prompts included policy advice, which podcasts to appear on and the definitions of various scientific terms. Peter Kyle's use of ChatGPT was defended by Sam Sharps of the Tony Blair Institute.\nIn 2024, the Government of the United Kingdom launched Gov.uk Chat to provide guidance on business rules and support. In 2025, the UK Government started to develop Humphrey, named after the character in Yes Minister, as a large language model tool for civil servants and the Cabinet Office expanded trials of its Redbox Copilot project.\n\n\n==== Scotland ====\nWhistleblowers have alleged that civil servants have written government policy papers with the assistance of ChatGPT.\n\n\n==== Wales ====\nIn 2023, MS Tom Giffard delivered a speech which had been written nearly completely using ChatGPT.\n\n\n=== United States ===\nIn 2025, OpenAI released ChatGPT Gov, a version of ChatGPT designed for federal government agencies.\nAccording to reporting by the Verge, tariffs in the second Trump administration may have been assigned based on a formula written using ChatGPT.\n\n\n==== California ====\nIn May 2024, Californian state agencies started to develop generative AI tools to solve common operational challenges.\n\n\n==== New York ====\nState lawmakers in New York passed legislation preventing agencies of the state government from replacing human workers with artificial intelligence.\n\n\n== References =="}
{"id": "6", "title": "Foundation model", "source": "https://en.wikipedia.org/wiki/Foundation_model", "text": "In artificial intelligence (AI), a foundation model (FM), also known as large X model (LxM), is a machine learning or deep learning model trained on vast datasets so that it can be applied across a wide range of use cases. Generative AI applications like large language models (LLM) are common examples of foundation models.\nBuilding foundation models is often highly resource-intensive, with the most advanced models costing hundreds of millions of dollars to cover the expenses of acquiring, curating, and processing massive datasets, as well as the compute power required for training. These costs stem from the need for sophisticated infrastructure, extended training times, and advanced hardware, such as GPUs. In contrast, adapting an existing foundation model for a specific task or using it directly is far less costly, as it leverages pre-trained capabilities and typically requires only fine-tuning on smaller, task-specific datasets.\nEarly examples of foundation models are language models (LMs) like OpenAI's GPT series and Google's BERT. Beyond text, foundation models have been developed across a range of modalities—including DALL-E and Flamingo for images, MusicGen for music, and RT-2 for robotic control. Foundation models are also being developed for fields like astronomy, radiology, genomics, music, coding, times-series forecasting, mathematics, and chemistry.\n\n\n== Definitions ==\nThe Stanford Institute for Human-Centered Artificial Intelligence's (HAI) Center for Research on Foundation Models (CRFM) coined the term \"foundation model\" in August 2021 to mean \"any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks\". This was based on their observation that preexisting terms, while overlapping, were not adequate, stating that \"'(large) language model' was too narrow given [the] focus is not only language; 'self-supervised model' was too specific to the training objective; and 'pretrained model' suggested that the noteworthy action all happened after 'pretraining.\" The term \"foundation model\" was chosen over \"foundational model\" because \"foundational\" implies that these models provide fundamental principles in a way that \"foundation\" does not.\nAs governments regulate foundation models, new legal definitions have emerged.\n\nIn the United States, the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence defines a foundation model as \"an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts\".\nIn the United States, the proposed AI Foundation Model Transparency Act of 2023 by House Representatives Don Beyer (D, VA) and Anna Eshoo (D, CA) defines a foundation model as \"an artificial intelligence model trained on broad data, generally uses self supervision, generally contains at least 1,000,000,000 parameters, is applicable across a wide range of contexts, and exhibits, or could be easily modified to exhibit, high levels of performance at tasks that could pose a serious risk to security, national economic security, national public health or safety, or any combination of those matters.\"\nIn the European Union, the European Parliament's negotiated position on the E.U. AI Act defines a foundation model as an \"AI model that is trained on broad data at scale, is designed for generality of output, and can be adapted to a wide range of distinctive tasks\".\nIn the United Kingdom, the Competition and Markets Authority's AI Foundation Models: Initial Report  defines foundations model as \"a type of AI technology that are trained on vast amounts of data that can be adapted to a wide range of tasks and operations.\"\nThe United States's definitions are the only ones to make reference to the size of a foundation model, and differ on magnitude. Beyer and Eshoo's definition also specifies that found"}
{"id": "7", "title": "Chinchilla (language model)", "source": "https://en.wikipedia.org/wiki/Chinchilla_(language_model)", "text": "Chinchilla is a family of large language models (LLMs) developed by the research team at Google DeepMind, presented in March 2022.\n\n\n== Models ==\nIt is named \"chinchilla\" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models. \nIt claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models, it has been determined that if one doubles the model size, one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost, Chinchilla has 70B parameters and four times as much data. \nChinchilla has an average accuracy of 67.5% on the Measuring Massive Multitask Language Understanding (MMLU) benchmark, which is 7% higher than Gopher's performance. Chinchilla was still in the testing phase as of January 12, 2023.\nChinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling, meaning that using larger, higher-quality training datasets can lead to better results on downstream tasks.\nIt has been used for the Flamingo vision-language model.\n\n\n== Architecture ==\nBoth the Gopher family and Chinchilla family are families of transformer models. \nIn particular, they are essentially the same as GPT-2, with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family, but trained with AdamW instead of Adam optimizer.\nThe Gopher family contains six models of increasing size, from 44 million parameters to 280 billion parameters. They refer to the largest one as \"Gopher\" by default. Similar naming conventions apply for the Chinchilla family.\nTable 1 of  shows the entire Gopher family:\n\nTable 4 of  compares the 70-billion-parameter Chinchilla with Gopher 280B.\n\n\n== See also ==\nLaMDA\n\n\n== References =="}
{"id": "8", "title": "Small language model", "source": "https://en.wikipedia.org/wiki/Small_language_model", "text": "Small language models or compact language models are artificial intelligence language models designed for human natural language processing including language and text generation. Unlike large language models, small language models are much smaller in scale and scope.  \nTypically, an large language models's number of training parameters is in the hundreds of billions, with some models even exceeding a trillion parameters. The size of any large language model is vast because it contains a large amount of information, which allows it to generate better content. However, this requires enormous computational power, making it impossible for an individual to train a large language model using just a single computer and graphical processing unit.\nSmall language models, on the other hand, use far fewer parameters, typically ranging from a few thousand to a few hundred million. This make them more feasible to train and host in resource-constrained environments such as a single computer or even a mobile device.\nMost contemporary (2020s) small language models use the same architecture as a large language model, but with a smaller parameter count and sometimes lower arithmetic precision. Parameter count is reduced by a combination of knowledge distillation and pruning. Precision can be reduced by quantization. Work on large language models mostly translate to small language models: pruning and quantization are also widely used to speed up large language models. Some notable models are:\n\nBelow 1B parameters: Llama-Prompt-Guard-2-22M (detects prompt injection and jailbreaking, based on DeBERTa-xsmall), SmolLM2-135M, SmolLM2-360M\n1–4B parameters: Llama3.2-1B, Qwen2.5-1.5B, DeepSeeek-R1-1.5B, SmolLM2-1.7B, SmolVLM-2.25B, Phi-3.5-Mini-3.8B, Phi-4-Mini-3.8B, Gemma3-4B; closed-weights ones include Gemini Nano\n4–14B parameters: Mistral 7B, Gemma 9B, Phi-4 14B.\n(Phi-4 14B is marginally \"small\" at best, but Microsoft does market it as a small model.)\n\n\n== Language model with small pre-training dataset ==\nTraditional AI language systems need enormous computers and vast amounts of data.  \nPre-training matters, even tiny models show significant performance improvements when pre-trained performance increases with larger pre-training datasets. Classification accuracy improves when pre-training and test datasets share similar tokens. Shallow architectures can replicate deep model performance through collaborative learning.\n\n\n== See also ==\nEdge computing\n\n\n== References =="}
{"id": "9", "title": "Modeling language", "source": "https://en.wikipedia.org/wiki/Modeling_language", "text": "A modeling language is a notation for expressing data, information or knowledge or systems in a structure that is defined by a consistent set of rules.\nA modeling language can be graphical or textual. A graphical modeling language uses a diagramming technique with named symbols that represent concepts and lines that connect the symbols and represent relationships and various other graphical notation to represent constraints. A textual modeling language may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions. An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS.\nNot all modeling languages are executable, and for those that are, the use of them doesn't necessarily mean that programmers are no longer required. On the contrary, executable modeling languages are intended to amplify the productivity of skilled programmers, so that they can address more challenging problems, such as parallel computing and distributed systems.\nA large number of modeling languages appear in the literature.\n\n\n== Type of modeling languages ==\n\n\n=== Graphical types ===\nExample of graphical modeling languages in the field of computer science, project management and systems engineering:\n\nBehavior Trees are a formal, graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system.\nBusiness Process Modeling Notation (BPMN, and the XML form BPML) is an example of a Process Modeling language.\nC-K theory consists of a modeling language for design processes.\nDRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems, a schematic representation of an algorithm or a stepwise process, and a family of programming languages.\nEXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language.\nExtended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers.\nFlowchart is a schematic representation of an algorithm or a stepwise process.\nFundamental Modeling Concepts (FMC) modeling language for software-intensive systems.\nIDEF is a family of modeling languages, which include IDEF0 for functional modeling, IDEF1X for information modeling, IDEF3 for business process modeling, IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies.\nJackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure.\nLePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java, C++, C#) programs and design patterns.\nLifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual, utilization, support and retirement stages.\nObject-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling, and can be used as a tool for information and rules analysis.\nPetri nets use variations on exactly one diagramming technique and topology, namely the bipartite graph.  The simplicity of its basic user interface easily enabled extensive tool support over the years, particularly in the areas of model checking, graphically oriented simulation, and software verification.\nSouthbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler's perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation.\nSpecification and Description Language (SDL) is a specification language targeted"}
{"id": "10", "title": "Claude (language model)", "source": "https://en.wikipedia.org/wiki/Claude_(language_model)", "text": "Claude is a family of large language models developed by Anthropic. The first model, Claude, was released in March 2023.\nThe Claude 3 family, released in March 2024, consists of three models: Haiku, optimized for speed; Sonnet, which balances capability and performance; and Opus, designed for complex reasoning tasks. These models can process both text and images, with Claude 3 Opus demonstrating enhanced capabilities in areas like mathematics, programming, and logical reasoning compared to previous versions.\nClaude 4, which includes Opus and Sonnet, was released in May 2025.\n\n\n== Training ==\nClaude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Then, they have been fine-tuned, notably using constitutional AI and reinforcement learning from human feedback (RLHF).\n\n\n=== Constitutional AI ===\nConstitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. The method, detailed in the paper \"Constitutional AI: Harmlessness from AI Feedback\" involves two phases: supervised learning and reinforcement learning.\nIn the supervised learning phase, the model generates responses to prompts, self-critiques these responses based on a set of guiding principles (a \"constitution\"), and revises the responses. Then the model is fine-tuned on these revised responses. For the reinforcement learning from AI feedback (RLAIF) phase, responses are generated, and an AI compares their compliance with this constitution. This dataset of AI feedback is used to train a preference model that evaluates responses based on how much they satisfy the constitution. Claude is then fine-tuned to align with this preference model. This technique is similar to RLHF, except that the comparisons used to train the preference model are AI-generated.\nThe constitution for Claude included 75 points, including sections from the UN Universal Declaration of Human Rights.\n\n\n== Models ==\n\nClaude is named after Claude Shannon, a pioneer in AI research.\n\n\n=== Claude ===\nClaude was the initial version of Anthropic's language model released in March 2023, Claude demonstrated proficiency in various tasks but had certain limitations in coding, math, and reasoning capabilities. Anthropic partnered with companies like Notion (productivity software) and Quora (to help develop the Poe chatbot).\n\n\n==== Claude Instant ====\nClaude was released as two versions, Claude and Claude Instant, with Claude Instant being a faster, less expensive, and lighter version. Claude Instant has an input context length of 100,000 tokens (which corresponds to around 75,000 words).\n\n\n=== Claude 2 ===\nClaude 2 was the next major iteration of Claude, which was released in July 2023 and available to the general public, whereas the Claude 1 was only available to selected users approved by Anthropic.\nClaude 2 expanded its context window from 9,000 tokens to 100,000 tokens. Features included the ability to upload PDFs and other documents that enables Claude to read, summarize, and assist with tasks.\n\n\n==== Claude 2.1 ====\nClaude 2.1 doubled the number of tokens that the chatbot could handle, increasing it to a window of 200,000 tokens, which equals around 500 pages of written material.\nAnthropic states that the new model is less likely to produce false statements compared to its predecessors.\n\n\n==== Criticism ====\nClaude 2 received criticism for its stringent ethical alignment that may reduce usability and performance. Users have been refused assistance with benign requests, for example with the system administration question \"How can I kill all python processes in my ubuntu server?\" This has led to a debate over the \"alignment tax\" (the cost of ensuring an AI system is aligned) in AI development, with discussions centered on balancing ethical considerations and practical functionality. Critics argued for user auto"}
{"id": "11", "title": "Generative pre-trained transformer", "source": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer", "text": "A generative pre-trained transformer (GPT) is a type of large language model (LLM) that is widely used in generative AI chatbots. GPTs are based on a deep learning architecture called the transformer. They are pre-trained on large data sets of unlabeled content, and able to generate novel content.\nOpenAI was the first to apply generative pre-training to the transformer architecture, introducing the GPT-1 model in 2018. The company has since released many bigger GPT models. The popular chatbot ChatGPT, released in late 2022 (using GPT-3.5), was followed by many competitor chatbots using their own \"GPT\" models to generate text, such as Gemini, DeepSeek or Claude.\nGPTs are primarily used to generate text, but can be trained to generate other kinds of data. For example, GPT-4o can process and generate text, images and audio. To improve performance on complex tasks, some GPTs, such as OpenAI o3, spend more time analyzing the problem before generating an output, and are called reasoning models. In 2025, GPT-5 was released with a router that automatically selects which model to use.\n\n\n== Background ==\nThe core technology of a GPT is the transformer architecture. Developed and introduced by Google researchers in the 2017 paper Attention Is All You Need, the transformer architecture solved many of the performance issues associated with older recurrent neural network (RNN) designs for natural language processing (NLP). The architecture's use of an attention mechanism allowed models to process entire sequences of text at once, enabling the training of much larger and more sophisticated models.\nSeparately, the concept of generative pre-training (GP) was a long-established technique in machine learning. GP is a form of self-supervised learning where a model is first trained on a large, unlabeled dataset (the \"pre-training\" step) to learn to generate data points. This pre-trained model is then adapted to a specific task using a labeled dataset (the \"fine-tuning\" step).\n\n\n== History ==\nIn June 2018, OpenAI published the paper Improving Language Understanding by Generative Pre-Training, which introduced the first generative pre-trained transformer model, GPT-1. This model combined the transformer architecture with generative pre-training, allowing it to be trained on large bodies of text (the BookCorpus) and then fine-tuned for a variety of specific language tasks. This semi-supervised approach was a breakthrough, as it reduced the need for large, manually-labeled datasets, which were expensive and time-consuming to create.\nOpenAI followed this with GPT-2 in 2019, a much larger model trained on a 40 GB dataset called WebText. Citing risks of malicious use, OpenAI initially opted for a \"staged release\", publishing smaller versions of the model before releasing the full 1.5-billion parameter model in November 2019. In 2020, GPT-3 was released with 175 billion parameters, trained on an even larger dataset. GPT-3 marked a significant leap in capability, demonstrating few-shot and zero-shot learning abilities where the model could perform tasks it was not explicitly trained for.\nOpenAI started using reinforcement learning from human feedback (RLHF) to better align the models' behavior with human preferences. This led to the development of \"InstructGPT\", a fine-tuned version of GPT-3, and ultimately the public release of the ChatGPT chatbot in November 2022. The immense popularity of ChatGPT spurred widespread development of competing GPT-based systems from other organizations. EleutherAI released a series of open-source models, including GPT-J in 2021. Other major technology companies developed their own large language models, including Google's PaLM and Meta AI's LLaMA.\nMany subsequent GPT models have been trained to be multimodal (able to process or generate multiple types of data). For example, GPT-4o can both process and generate text, images and audio. Additionally, GPT models like o3 or DeepSeek R1 have been trained with reinforcement learni"}
{"id": "12", "title": "1.58-bit large language model", "source": "https://en.wikipedia.org/wiki/1.58-bit_large_language_model", "text": "A 1.58-bit large language model (also known as a ternary LLM) is a type of large language model (LLM) designed to be computationally efficient. It achieves this by using weights that are restricted to only three values: -1, 0, and +1. This restriction significantly reduces the model's memory footprint and allows for faster processing, as complex multiplication operations can be replaced with simpler additions. This contrasts with traditional models that use 16-bit floating-point numbers (FP16 or BF16) for their weights.\nStudies have shown that for models up to several billion parameters, the performance of 1.58-bit LLMs on various tasks is comparable to their full-precision counterparts. This approach could enable powerful AI to run on less specialized and lower-power hardware.\nThe name \"1.58-bit\" comes from the fact that a system with three states contains \n  \n    \n      \n        \n          log\n          \n            2\n          \n        \n        ⁡\n        3\n        ≈\n        1.58\n      \n    \n    {\\displaystyle \\log _{2}3\\approx 1.58}\n  \n bits of information. These models are sometimes also referred to as 1-bit LLMs in research papers, although this term can also refer to true binary models (with weights of -1 and +1).\n\n\n== BitNet ==\n\nIn 2024, Ma et al., researchers at Microsoft, declared that their 1.58-bit model, BitNet b1.58 is comparable in performance to the 16-bit Llama 2 and opens the era of 1-bit LLM. BitNet creators did not use the post-training quantization of weights  but instead relied on the new BitLinear transform that replaced the nn.Linear layer of the traditional transformer design.\nIn 2025, Microsoft researchers had released an open-weights and open inference code model BitNet b1.58 2B4T demonstrating performance competitive with the full precision models at 2B parameters and 4T training tokens.\n\n\n== Post-training quantization ==\nBitNet derives its performance from being trained natively in 1.58 bit instead of being quantized from a full-precision model after training. Still, training is an expensive process and it would be desirable to be able to somehow convert an existing model to 1.58 bits. In 2024, HuggingFace reported a way to gradually ramp up the 1.58-bit quantization in fine-tuning an existing model down to 1.58 bits.\n\n\n== Critique ==\nSome researchers point out that the scaling laws of large language models favor the low-bit weights only in case of undertrained models. As the number of training tokens increases, the deficiencies of low-bit quantization surface.\n\n\n== References ==\n\n\n== Sources ==\nMa, Shuming; Wang, Hongyu; Ma, Lingxiao; Wang, Lei; Wang, Wenhui; Huang, Shaohan; Dong, Li; Wang, Ruiping; Xue, Jilong; Wei, Furu (2024-02-27). \"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\". arXiv:2402.17764 [cs.CL].\nMa, Shuming; Wang, Hongyu; Huang, Shaohan; Zhang, Xingxing; Hu, Ying; Song, Ting; Xia, Yan; Wei, Furu (2025). \"BitNet b1.58 2B4T Technical Report\". arXiv:2504.12285 [cs.CL].\nFriha, Othmane; Amine Ferrag, Mohamed; Kantarci, Burak; Cakmak, Burak; Ozgun, Arda; Ghoualmi-Zine, Nassira (2024). \"LLM-Based Edge Intelligence: A Comprehensive Survey on Architectures, Applications, Security and Trustworthiness\". IEEE Open Journal of the Communications Society. 5: 5799–5856. doi:10.1109/OJCOMS.2024.3456549. ISSN 2644-125X.\nHutson, Matthew (2024-05-30). \"1-bit LLMs Could Solve AI's Energy Demands\". IEEE Spectrum. Retrieved 2025-04-22.\nHuyen, Chip (2024-12-04). AI Engineering. \"O'Reilly Media, Inc.\". ISBN 978-1-0981-6627-4. Retrieved 2025-04-22.\nKumar, Tanishq; Ankner, Zachary; Spector, Benjamin F.; Bordelon, Blake; Muennighoff, Niklas; Paul, Mansheej; Pehlevan, Cengiz; Ré, Christopher; Raghunathan, Aditi (2024). \"Scaling Laws for Precision\". arXiv:2411.04330 [cs.LG].\nMorales, Jowi (2025-04-17). \"Microsoft researchers build 1-bit AI LLM with 2B parameters\". Tom's Hardware. Retrieved 2025-04-21.\nOuyang, Xu; Ge, Tao; Hartvigsen, Thomas; Zhang, Zhisong; Mi, Haitao; Yu, Dong (2024). \"Low-Bit"}
{"id": "13", "title": "Gemini (language model)", "source": "https://en.wikipedia.org/wiki/Gemini_(language_model)", "text": "Gemini is a family of multimodal large language models (LLMs) developed by Google DeepMind, and the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI's GPT-4. It powers the chatbot of the same name. In March 2025, Gemini 2.5 Pro Experimental was rated as highly competitive.\n\n\n== History ==\n\n\n=== Development ===\n\nGoogle announced Gemini, a large language model (LLM) developed by subsidiary Google DeepMind, during the Google I/O keynote on May 10, 2023. It was positioned as a more powerful successor to PaLM 2, which was also unveiled at the event, with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages. Unlike other LLMs, Gemini was said to be unique in that it was not trained on a text corpus alone and was designed to be multimodal, meaning it could process multiple types of data simultaneously, including text, images, audio, video, and computer code. It had been developed as a collaboration between DeepMind and Google Brain, two branches of Google that had been merged as Google DeepMind the previous month. In an interview with Wired, DeepMind CEO Demis Hassabis touted Gemini's advanced capabilities, which he believed would allow the algorithm to trump OpenAI's ChatGPT, which runs on GPT-4 and whose growing popularity had been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind's AlphaGo program, which gained worldwide attention in 2016 when it defeated Go champion Lee Sedol, saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.\nIn August 2023, The Information published a report outlining Google's roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard, Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini, along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a \"core contributor\" to Gemini. Because Gemini was being trained on transcripts of YouTube videos, lawyers were brought in to filter out any potentially copyrighted materials.\nWith news of Gemini's impending launch, OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to \"an early version\" of the LLM, which Google intended to make available to clients through Google Cloud's Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft's GitHub Copilot.\n\n\n=== Launch ===\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference. It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, designed for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\". At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2. It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the"}
{"id": "14", "title": "Model collapse", "source": "https://en.wikipedia.org/wiki/Model_collapse", "text": "Model collapse is a phenomenon where machine learning models gradually degrade due to errors coming from uncurated training on the outputs of another model, such as prior versions of itself. Such outputs are known as synthetic data. It is a possible mechanism for mode collapse.\nShumailov et al. coined the term and described two specific stages to the degradation: early model collapse and late model collapse:\n\nIn early model collapse, the model begins losing information about the tails of the distribution – mostly affecting minority data. Later work highlighted that early model collapse is hard to notice, since overall performance may appear to improve, while the model loses performance on minority data.\nIn late model collapse, the model loses a significant proportion of its performance, confusing concepts and losing most of its variance.\n\n\n== Mechanism ==\nUsing synthetic data as training data can lead to issues with the quality and reliability of the trained model. Model collapse occurs for three main reasons:\n\nfunctional approximation errors\nsampling errors\nlearning errors\nImportantly, it happens in even the simplest of models, where not all of the error sources are present. In more complex models the errors often compound, leading to faster collapse.\n\n\n== Disagreement over real-world impact ==\n\nSome researchers and commentators on model collapse warn that the phenomenon could fundamentally threaten future generative AI development: As AI-generated data is shared on the Internet, it will inevitably end up in future training datasets, which are often crawled from the Internet. If training on \"slop\" (large quantities of unlabeled synthetic data) inevitably leads to model collapse, this could therefore pose a difficult problem.\nHowever, recently, other researchers have disagreed with this argument, showing that if synthetic data accumulates alongside human-generated data, model collapse is avoided. The researchers argue that data accumulating over time is a more realistic description of reality than deleting all existing data every year, and that the real-world impact of model collapse may not be as catastrophic as feared. \nAn alternative branch of the literature investigates the use of machine learning detectors and watermarking to identify model generated data and filter it out.\n\n\n== Mathematical models of the phenomenon ==\n\n\n=== 1D Gaussian model ===\nIn 2024, a first attempt has been made at illustrating collapse for the simplest possible model — a single dimensional normal distribution fit using unbiased estimators of mean and variance, computed on samples from the previous generation. \nTo make this more precise, we say that original data follows a normal distribution \n  \n    \n      \n        \n          X\n          \n            0\n          \n        \n        ∼\n        \n          \n            N\n          \n        \n        (\n        μ\n        ,\n        \n          σ\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle X^{0}\\sim {\\mathcal {N}}(\\mu ,\\sigma ^{2})}\n  \n, and we possess \n  \n    \n      \n        \n          M\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle M_{0}}\n  \n samples \n  \n    \n      \n        \n          X\n          \n            j\n          \n          \n            0\n          \n        \n      \n    \n    {\\displaystyle X_{j}^{0}}\n  \n for \n  \n    \n      \n        j\n        ∈\n        \n          {\n          \n          1\n          ,\n          …\n          ,\n          \n            M\n            \n              0\n            \n          \n          \n          \n\n          \n          }\n        \n      \n    \n    {\\displaystyle j\\in {\\{\\,1,\\dots ,M_{0}\\,{}\\}}}\n  \n. Denoting a general sample \n  \n    \n      \n        \n          X\n          \n            j\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{j}^{i}}\n  \n as sample \n  \n    \n      \n        j\n        ∈\n        \n          {\n          \n          1\n          ,\n          …\n          ,"}
{"id": "15", "title": "T5 (language model)", "source": "https://en.wikipedia.org/wiki/T5_(language_model)", "text": "T5 (Text-to-Text Transfer Transformer) is a series of large language models developed by Google AI introduced in 2019. Like the original Transformer model, T5 models are encoder-decoder Transformers, where the encoder processes the input text, and the decoder generates the output text.\nT5 models are usually pretrained on a massive dataset of text and code, after which they can perform the text-based tasks that are similar to their pretrained tasks. They can also be finetuned to perform other tasks.\nT5 models have been employed in various applications, including chatbots, machine translation systems, text summarization tools, code generation, and robotics.\n\n\n== Training ==\nThe original T5 models are pre-trained on the Colossal Clean Crawled Corpus (C4), containing text and code scraped from the internet. This pre-training process enables the models to learn general language understanding and generation abilities. T5 models can then be fine-tuned on specific downstream tasks, adapting their knowledge to perform well in various applications.\nThe T5 models were pretrained on many tasks, all in the format of <input text> -> <output text>.\n\nSome examples are:\n\nrestoring corrupted text: Thank you <X> me to your party <Y> week. -> <X> for inviting <Y> last <Z>, where the <Z> means \"end of output\", and the  <X> and  <Y> denote blanks to be filled, called \"sentinels\" in the original report.\ntranslation: translate English to German: That is good. -> Das ist gut..\njudging the grammatical acceptability of a sentence (CoLA sentence): The course is jumping well. -> not acceptable .\n\n\n== Architecture ==\n\nThe T5 series encompasses several models with varying sizes and capabilities, all encoder-decoder Transformers, where the encoder processes the input text, and the decoder generates the output text.\nThese models are often distinguished by their parameter count, which indicates the complexity and potential capacity of the model. The original paper reported the following 5 models:\n\n*The encoder and the decoder have the same shape. So for example, the T5-small has 6 layers in the encoder and 6 layers in the decoder.\nIn the above table,\n\n  \n    \n      \n        \n          n\n          \n            layer\n          \n        \n      \n    \n    {\\displaystyle n_{\\text{layer}}}\n  \n: Number of layers in the encoder; also, number of layers in the decoder. They always have the same number of layers.\n\n  \n    \n      \n        \n          n\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle n_{\\text{head}}}\n  \n: Number of attention heads in each attention block.\n\n  \n    \n      \n        \n          d\n          \n            model\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{model}}}\n  \n: Dimension of the embedding vectors.\n\n  \n    \n      \n        \n          d\n          \n            ff\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{ff}}}\n  \n: Dimension of the feedforward network within each encoder and decoder layer.\n\n  \n    \n      \n        \n          d\n          \n            kv\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{kv}}}\n  \n: Dimension of the key and value vectors used in the self-attention mechanism.\nNote that unlike typical Transformers, the 3B and 11B models do not satisfy \n  \n    \n      \n        \n          d\n          \n            model\n          \n        \n        =\n        \n          d\n          \n            kv\n          \n        \n        \n          n\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{model}}=d_{\\text{kv}}n_{\\text{head}}}\n  \n.\nCompared to the original Transformer, it uses a few minor modifications: layer normalization with no additive bias; placing the layer normalization outside the residual path; relative positional embedding.\nFor all experiments, they used a WordPiece tokenizer, with vocabulary size 32,000. The tokenizer is shared across both the input and output of each model. It was trained on a mixture of English, German, Fre"}
{"id": "16", "title": "Language model benchmark", "source": "https://en.wikipedia.org/wiki/Language_model_benchmark", "text": "Language model benchmark is a standardized test designed to evaluate the performance of language model on various natural language processing tasks. These tests are intended for comparing different models' capabilities in areas such as language understanding, generation, and reasoning.\nBenchmarks generally consist of a dataset and corresponding evaluation metrics. The dataset provides text samples and annotations, while the metrics measure a model's performance on tasks like question answering, text classification, and machine translation. These benchmarks are developed and maintained by academic institutions, research organizations, and industry players to track progress in the field.\n\n\n== Overview ==\n\n\n=== Types ===\nBenchmarks may be described by the following adjectives, not mutually exclusive:\n\nClassical: These tasks are studied in natural language processing, even before the advent of deep learning. Examples include the Penn Treebank for testing syntactic and semantic parsing, as well as bilingual translation benchmarked by BLEU scores.\nQuestion answering: These tasks have a text question and a text answer, often multiple-choice. They can be open-book or closed-book. Open-book QA resembles reading comprehension questions, with relevant passages included as annotation in the question, in which the answer appears. Closed-book QA includes no relevant passages. Closed-book QA is also called open-domain question-answering. Before the era of large language models, open-book QA was more common, and understood as testing information retrieval methods. Closed-book QA became common since GPT-2 as a method to measure knowledge stored within model parameters.\nOmnibus: An omnibus benchmark combines many benchmarks, often previously published. It is intended as an all-in-one benchmarking solution.\nReasoning: These tasks are usually in the question-answering format, but are intended to be more difficult than standard question answering.\nMultimodal: These tasks require processing not only text, but also other modalities, such as images and sound. Examples include OCR and transcription.\nAgency: These tasks are for a language-model–based software agent that operates a computer for a user, such as editing images, browsing the web, etc.\nAdversarial: A benchmark is \"adversarial\" if the items in the benchmark are picked specifically so that certain models do badly on them. Adversarial benchmarks are often constructed after SOTA models have saturated a benchmark, to renew the benchmark. A benchmark is \"adversarial\" only at a certain moment in time, since what is adversarial may cease to be adversarial as newer SOTA models appear.\nPublic/Private: A benchmark might be partly or entirely private, meaning that some or all of the questions are not publicly available. The idea is that if a question is publicly available, then it might be used for training, which would be \"training on the test set\" and invalidate the result of the benchmark. Usually, only the guardians of the benchmark has access to the private subsets, and to score a model on such a benchmark, one must send the model weights, or provide API access, to the guardians.\nThe boundary between a benchmark and a dataset is not sharp. Generally, a dataset contains three \"splits\": training, test, validation. Both the test and validation splits are essentially benchmarks. In general, a benchmark is distinguished from a test/validation dataset in that a benchmark is typically intended to be used to measure the performance of many different models that are not trained specifically for doing well on the benchmark, while a test/validation set is intended to be used to measure the performance of models trained specifically on the corresponding training set. In other words, a benchmark may be thought of as a test/validation set without a corresponding training set.\nConversely, certain benchmarks may be used as a training set, such as the English Gigaword or the One Billion Word Benchmark, which in mo"}
{"id": "17", "title": "Mistral AI", "source": "https://en.wikipedia.org/wiki/Mistral_AI", "text": "Mistral AI SAS (French: [mistʁal]) is a French artificial intelligence (AI) startup, headquartered in Paris. Founded in 2023, it specializes in open-weight large language models (LLMs), with both open-source and proprietary AI models.\n\n\n== Namesake ==\nThe company is named after the mistral, a powerful, cold wind in southern France.\n\n\n== History ==\nMistral AI was established in April 2023 by three French AI researchers, Arthur Mensch, Guillaume Lample and Timothée Lacroix.\nMensch, an expert in advanced AI systems, is a former employee of Google DeepMind; Lample and Lacroix, meanwhile, are large-scale AI models specialists who had worked for Meta Platforms.\nThe trio originally met during their studies at École Polytechnique.\n\n\n== Company operation ==\n\n\n=== Funding ===\nIn June 2023, the start-up carried out a first fundraising of €105 million ($117 million) with investors including the American fund Lightspeed Venture Partners, Eric Schmidt, Xavier Niel and JCDecaux. The valuation is then estimated by the Financial Times at €240 million ($267 million).\nOn 10 December 2023, Mistral AI announced that it had raised €385 million ($428 million) as part of its second fundraising. This round of financing involves the Californian fund Andreessen Horowitz, BNP Paribas and the software publisher Salesforce.\nBy December 2023, it was valued at over $2 billion.\nOn 16 April 2024, reporting revealed that Mistral was in talks to raise €500 million, a deal that would more than double its current valuation to at least €5 billion.\nIn June 2024, Mistral AI secured a €600 million ($645 million) funding round, elevating its valuation to €5.8 billion ($6.2 billion).\nLed by venture capital firm General Catalyst, this round resulted in additional contributions from existing investors. The funds aim to support the company's expansion.\nBased on valuation, as of June 2024, the company is ranked fourth globally in the AI industry, and first outside the San Francisco Bay Area.\nIn early August 2025, the Financial Times reported that Mistral was in talks to raise $1 billion at a $10 billion valuation.\n\n\n=== Partnerships ===\nOn 26 February 2024, Microsoft announced that Mistral's language models would be made available on Microsoft's Azure cloud, while the multilingual conversational assistant Le Chat would be launched in the style of ChatGPT. The partnership also included a financial investment of $16 million by Microsoft in Mistral AI.\nIn April 2025, Mistral AI announced a €100 million partnership with the shipping company CMA CGM.\n\n\n== Services ==\nOn November 19, 2024, the company announced updates for Le Chat (pronounced /lə tʃat/ in French).\nIt added the ability to create images, using Black Forest Labs' Flux Pro model.\nOn February 6, 2025, Mistral AI released Le Chat on iOS and Android mobile devices.\nMistral AI also introduced a Pro subscription tier, priced at $14.99 per month, which provides access to more advanced models, unlimited messaging, and web browsing.\n\n\n== Models ==\nThe following table lists the main model versions of Mistral, describing the significant changes included with each version:\n\n\n=== Mistral 7B ===\nMistral AI claimed in the Mistral 7B release blog post that the model outperforms LLaMA 2 13B on all benchmarks tested, and is on par with LLaMA 34B on many benchmarks tested, despite having only 7 billion parameters, a small size compared to its competitors.\n\n\n=== Mixtral 8x7B ===\nMistral AI's testing in 2023 shows the model beats both LLaMA 70B, and GPT-3.5 in most benchmarks.\nIn March 2024, a research conducted by Patronus AI comparing performance of LLMs on a 100-question test with prompts to generate text from books protected under U.S. copyright law found that Open AI's GPT-4, Mixtral, Meta AI's LLaMA-2, and Anthropic's Claude 2 generated copyrighted text verbatim in 44%, 22%, 10%, and 8% of responses respectively.\n\n\n=== Mistral Small 3.1 ===\nOn 17 March 2025, Mistral released Mistral Small 3.1 as a smaller, more efficient model."}
{"id": "18", "title": "Generative artificial intelligence", "source": "https://en.wikipedia.org/wiki/Generative_artificial_intelligence", "text": "Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, xAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu. \nGenerative AI is used across many industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. The production of Generative AI systems requires large scale data centers using specialized chips which require high levels of energy for processing and water for cooling. \nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.  The material and energy intensity of the AI systems has raised concerns about the environmental impact of AI, especially in light of the challenges created by the energy transition.\n\n\n== History ==\n\n\n=== Early history ===\nThe first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is trained on a text corpus, it can then be used as a probabilistic text generator.\nComputers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\n\n\n=== Generative neural networks (2014–2019) ===\n\nSince its inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire ima"}
{"id": "19", "title": "BERT (language model)", "source": "https://en.wikipedia.org/wiki/BERT_(language_model)", "text": "Bidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. BERT dramatically improved the state-of-the-art for large language models. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments. \nBERT is trained by masked token prediction and next sentence prediction. As a result of this training process, BERT learns contextual, latent representations of tokens in their context, similar to ELMo and GPT-2. It found applications for many natural language processing tasks, such as coreference resolution and polysemy resolution. It is an evolutionary step over ELMo, and spawned the study of \"BERTology\", which attempts to interpret what is learned by BERT.\nBERT was originally implemented in the English language at two model sizes, BERTBASE (110 million parameters) and BERTLARGE (340 million parameters). Both were trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words). The weights were released on GitHub. On March 11, 2020, 24 smaller models were released, the smallest being BERTTINY with just 4 million parameters.\n\n\n== Architecture ==\n\nBERT is an \"encoder-only\" transformer architecture. At a high level, BERT consists of 4 modules: \n\nTokenizer: This module converts a piece of English text into a sequence of integers (\"tokens\").\nEmbedding: This module converts the sequence of tokens into an array of real-valued vectors representing the tokens. It represents the conversion of discrete token types into a lower-dimensional Euclidean space.\nEncoder: a stack of Transformer blocks with self-attention, but without causal masking.\nTask head: This module converts the final representation vectors into one-hot encoded tokens again by producing a predicted probability distribution over the token types. It can be viewed as a simple decoder, decoding the latent representation into token types, or as an \"un-embedding layer\".\nThe task head is necessary for pre-training, but it is often unnecessary for so-called \"downstream tasks,\" such as question answering or sentiment classification. Instead, one removes the task head and replaces it with a newly initialized module suited for the task, and finetune the new module. The latent vector representation of the model is directly fed into this new module, allowing for sample-efficient transfer learning.\n\n\n=== Embedding ===\nThis section describes the embedding used by BERTBASE. The other one, BERTLARGE, is similar, just larger.\nThe tokenizer of BERT is WordPiece, which is a sub-word strategy like byte-pair encoding. Its vocabulary size is 30,000, and any token not appearing in its vocabulary is replaced by [UNK] (\"unknown\"). \n\nThe first layer is the embedding layer, which contains three components: token type embeddings, position embeddings, and segment type embeddings. \n\nToken type: The token type is a standard embedding layer, translating a one-hot vector into a dense vector based on its token type.\nPosition: The position embeddings are based on a token's position in the sequence. BERT uses absolute position embeddings, where each position in a sequence is mapped to a real-valued vector. Each dimension of the vector consists of a sinusoidal function that takes the position in the sequence as input.\nSegment type: Using a vocabulary of just 0 or 1, this embedding layer produces a dense vector based on whether the token belongs to the first or second text segment in that input. In other words, type-1 tokens are all tokens that appear after the [SEP] special token. All prior tokens are type-0.\nThe three embedding vectors are added together representing the initial token representation as a function of these three pieces of information. After embedding, the vector representation is normalized using a LayerNorm operation, outputting a 768-dim"}
